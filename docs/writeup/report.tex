\documentclass[english]{report}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{babel}

% R code colorization
% based on http://stackoverflow.com/questions/21402157/colour-for-r-code-chunk-in-listings-package/21468454#21468454
\usepackage{listings}             % Include the listings-package
\usepackage[usenames,dvipsnames]{color}    
 \lstset{ 
  language=R,                     % the language of the code
  basicstyle=\ttfamily, % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{Blue},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{RoyalBlue},      % keyword style
  commentstyle=\color{YellowGreen},   % comment style
  stringstyle=\color{ForestGreen}      % string literal style
} 

\begin{document}

\title{Stat440 Project Outline}
\author{Daniel Galperin, Nick Guenther}
\date{} % disable the autodate. we can put it back if we want it.


% this should be 10 pages: 1 page title, 1 page references, 8 pages of content

% 1 page
\maketitle
\newpage

\tableofcontents %because latex is from before the days of interactive computing, you need to run the build twice for this to show up
 % also note that only autonumbered sections get entered into the TOC by default
 % see http://www.andy-roberts.net/writing/latex/contents

%these macros correct part of the oversight:
% of couse, these macros also confuse texmaker
% le sigh
\newcommand{\Section}[1] {
  \section*{#1}
  \addcontentsline{toc}{section}{#1}
}

\newcommand{\Subsection}[1] {
  \subsection*{#1}
  \addcontentsline{toc}{subsection}{#1}
}

\newcommand{\Subsubsection}[1] {
  \subsubsection*{#1}
  \addcontentsline{toc}{subsubsection}{#1}
}

% .5 pages
\Section{Introduction}
% some of section below from wikipedia
The Normal-Inverse Wishart distribution is a multivariate four-parameter family of continous probability distributions.
%.....

%its history
%its relationship to other others (it's a generalized Normal-Inverse-ChiSq)

%In bayesian statistics, it is the conjugate prior in (...this situation...)

Its main use is as the conjugate prior for a multivariate normal distribution with unknown mean and covariance matrix.

%Due to its common use in bayesian statistics, Generating samples from this distribution is often the bottleneck in generating the %credible interval. 

Unlike for normal or $\chi^2$, there is no built in function in R to generate samples. Our goal is to provide such functionality for R, and as generating samples is often the bottleneck in %need to figure out what to call this% 
	we intended to do it as efficiently as possible.
% (4 pages)
\Section{Program Package}


\begin{lstlisting}[frame=single, language=R]
# test.MultivariableRegression.R
#
#

# originally n = 27, m = 1e5
test.EversonMorris <- function(n=27, m=1e7) {
  # Smoketest for lm.multivariable
  #
  # simulate multivariable normal test data
  # then see if the bayesian fitter can pull out the
  # correct (artificial and frequentist) coefficients.
  # Parameters based on Everson & Morris [2000]
  #
  # Note: the proper Everson Morris paper used a *random effects* model which
  #  turns out(?) to be algebraically equivalent to a multivariable model.
  #  This function only uses the multivariable model directly.
  #
  # args:
  #  n: number of observed samples to take
  #     the default is purposely small, to reflect
  #     a realistic data-gathering situation.
  #  m: number of posterior samples to take
  #
  # returns:
  #  nothing; instead, results are printed as work is done.

  d = 1;
  q = 2;
  A = matrix(c(3.38,-.77,-.77,2.55), q, q)
  B = matrix(c(0,0), d, q)
	
  message("True coefficients")
  print(B)
  message("True covariance")
  print(A)
  
  data = rmultivariableregression(n, B, A);
  
  message("Hiding true values from ourselves")
  rm(B, A, d, q)
  
  message("Data is:")
  message("Y:")
  print(data$Y)
  message("X:")
  print(data$X)

  X.sq <- crossprod(data$X);
  beta.hat1 <- solve(X.sq, t(data$X) %*% data$Y);
  message("beta hat point estimate")
  print(beta.hat1)
  
  message("Sampling posterior distribution")
  #  m = 2 #DEBUG
  result <- lm.multivariable(m, data$X, data$Y)
 
  d = dim(result$B)[1]
  q = dim(result$V)[1]
  
  message("Recovered dimensionality: q = ", q, " and d = ", d)
  message("Estimated B")
  B.hat = apply(result$B, -3, mean) #take mean()s across the 3rd dimension;
                            # results in a q x d matrix (after unflattening apply's results)
  dim(B.hat) = c(q,d)
  print(B.hat)
  message() #newline

  message("Estimated V")
  V.hat = apply(result$V, -3, mean) #take mean()s across the 3rd dimension;
                            # results in a q x q matrix (or it would, but apply flattens its results)
  dim(V.hat) = c(q,q)
  print(V.hat)
  message() #newline
  
  # TODO: compute (with 'quantile') the confidence intervals for each B and V
  #
  # additionally, we have a whole sample from which we can do bootstrap-like things, compute functions of the data, etc
  # but for this simple test, getting the right coefficients is good enough.
  list(V = result$V, B = result$B, X = data$X, Y = data$Y)
}
\end{lstlisting}

% 1 page
\Subsection{Algorithm}

- Why we care
- State of the art
  - MCMCpack
  ```
  > riwish
  function (v, S) 
  {
      return(solve(rwish(v, solve(S))))
      }
   ```   
 there's also rinvwishartc and dinvwishartc in LaplacesDemon(http://www.bayesian-inference.com/softwaredownload) 
   AND there's a version which is cholesky-parameterized
  but their version strictly only does one random matrix at a time
  
- What we did differently

% (2?? pages)
\Subsection{Testing}

% 1 pages
\Subsubsection{Correctness <-- this title is dumb}

There is no test we can do which will demonstrate the correctness of this code.
Like Knuth, we have only proven this code correct, not tested it thoroughly.
But there are some relatively basic tests we have applied to 
(- brief mention that doing a full multivariate K-S test is so hard as to be ridiculous (+ citation?))

Each element we are generating has a distribution by itself. These are the \emph{marginal distributions} such as:

$$ V_{2,3} | X_1, X_2, X_3, ..., V_{1,1}, V_{1,2}, V_{1,3} , ....  \sim P(V_{2,3} = v) $$
% BUG: continuous distributions don't have "equals"
%  I wish there was a consistent notation which covered the discrete and continuous cases together without having to jump a layer of indirection into pdfs

We can derive the marginals directly. For the elements of $X$, we have:


so  $X_i \sim t_{???}$ .

For the elements of $V$, we have:

....


More quickly, we can sample $X,V \sim NIW$ from a sampler that we know should work: the naive algorithm %TODO: <-- grammar%
  , and then use kernel density estimation.
  
With either method, we get a PDF, and we can plot histograms and the PDF together. Here are some examples of this from our final code:

(IMAGE: A correct sample)
(IMAGE: An incorrect sample)


Every \emph{moment} is an expected value. This makes moments an easy to compute statistic,
 because (for $ S_i \sim S $)
$$ lim_{n \rightarrow \infty} \frac{\sum_{i=1}^n g(S_i)}{n} = E[g(S)] $$
  ((TODO: prove. why is this? the CLT??))
so we can just take samples, map them with a function, and average. As more samples come in we expect these numbers to converge to specific values. As before, we can examine the marginals one by one. We can check for convergence by.

Our moments are multivariate, so as before we need to look at marginals 
(IMAGE: an element moment converging to the expected value)

Finally, we used a k-s test [CITE?] to numerically detect marginals which deviate from the correct value. Example 
```
snappy2 X[3]: different
snappy2 X[4]: same
snappy2 V[1,1]: same
snappy2 V[1,2]: same
snappy2 V[1,3]: different
```


If, under a large number of samples, any of these statistics ((is it correct to call a full pdf a 'statistic'? you could view a function as an infinite set related statistics...)) do not match its expected value--pdf or moment--it means there is an error. Conversely, while all of the statistics matching doesn't logically prove correctness, they give strong support to likelihood of correctness.



% 1 page
\Subsubsection{Benchmarking}

- methodology
- algorithms compared
  - naive
  - snappy in R
  - snappy in Rcpp
% - snappy in python (on numpy)
%  - snappy in cython (on numpy)

These results show that our algorithm has approximately a (10?) times speed up over the naive algorithm, with a further (3?) times speed up by the port to C.

% (4 pages?)
\Section{Applications}

The NIW distribution is a extremely common conjugate prior. Here, we use our algorithm to rapidly do two simulation studies:


% 1.25 pages
\Subsection{Multivariable Regression}

One use of the NIW distribution is to get distributions and credible intervals for parameters and variance in multivariable regression.\\

Let $X_{n\times p} = [x_{ij}]$ and $Y_{n\times q} = [y_{ij}]$ be matrices of p predictors and q respones for each of n observations, and let $E_{n\times q}$ be the matrix of errors. The regression model is

\[Y = X\beta + E, (\epsilon_{i1},\hdots,\epsilon_{i q})\sim N_q(0,V)\]

where $\beta_{p\times q} = [\beta_{ij}] $ and $V_{q\times q}$. The conjugate prior for the model is:

\begin{align*}
	V &\sim  Inv-W_q(\Psi,\nu)\\
	\beta|V &\sim N_{pq}\{\Lambda, V  \otimes \Omega^{-1}\}
\end{align*}

The parameters of the prior are $\Psi_{q\times q}, \nu_{1\times 1}, \Lambda_{p\times q}, \Omega_{p\times p}$\\

The resulting posterior distribution is.


\begin{align*}
	V|Y,X &\sim  Inv\text{-}W_q(\Psi+S+C,\nu+n)\\
	\beta|V,Y,X &\sim N_{pq}\{A\Lambda + (I-A)\hat{\beta}, V  \otimes (X'X +\Omega)^{-1}\}
\end{align*}
where 
\begin{align*}
\hat{\beta} &= (X'X)^{-1}X'Y\\
S &= (Y-X\hat{\beta})(Y-X\hat{\beta})\\ 
A &= (X'X + \Omega)^{-1}\Omega\\
C &= \hat{\beta}'(X'X)\hat{\beta} + \Lambda'\Omega\Lambda - (X'X\hat{\beta} + \Omega\Lambda)'(X'X+\Omega)^{-1}(X'X\hat{\beta} + \Omega\Lambda)
\end{align*}

The following code generate B and V given $X, Y, \Lambda, \Omega, \nu, \Psi$

% Insert R code here? alternatively just say algorithm does it.

\Subsubsection{specific example}

Using data from 

% 1.25 pages
%\Subsection{Gibbs Sampler}

%- we did this thing
%- here's our test case
%- and look, the code is reusable (and in C)

% .5 pages
\Section{Conclusion}

blah blah blah yay code we like R

((APPENDIX??!?!?! WITH THE CODE AND STUFF YEAH))

% 1 page
\newpage
\Section{References}

- a ref to the NIW dist
- Rcpp

\end{document}
